**DistilBERT** is a smaller and faster variant of the BERT (Bidirectional Encoder Representations from Transformers) model introduced by Hugging Face in 2019. It is designed to be more lightweight and efficient while retaining much of the performance of the original BERT model.

<br>

- [Models weights]([https://github.com/huggingface/transformers](https://drive.google.com/drive/folders/1s87g_3wwcu7na00Z0t43AyyqFPyfHfny?usp=sharing))
