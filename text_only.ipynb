{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beating Text-Graph-17 with only Text\n",
    "\n",
    "Current plan is following\n",
    "\n",
    "- preprocess text into **q-a connection prediction** (question + question entities [SEP] answer + answer entities (+ linear. graph))\n",
    "- finetune bert-like model (bigger=better) with some cool LoRA (this one needs to be tuned too)\n",
    "- abuse augmentations for upsampling minor \"correct\" label examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.random.manual_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.cuda.random.manual_seed(SEED)\n",
    "torch.cuda.random.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/tsv/train.tsv'\n",
    "test_path = 'data/tsv/test.tsv'\n",
    "\n",
    "def linearize_graph(graph_dict, sep_token):\n",
    "    \"\"\"Borrowed from baseline, needs to be modified...\"\"\"\n",
    "    \n",
    "    nodes = sorted((node_dict for node_dict in graph_dict[\"nodes\"]), key=lambda d:d[\"id\"])\n",
    "    for n_id, node_dict in enumerate(nodes):\n",
    "        assert n_id == node_dict[\"id\"]\n",
    "    src_node_id2links = {}\n",
    "    \n",
    "    for link_dict in graph_dict[\"links\"]:\n",
    "        link_src =  link_dict[\"source\"]\n",
    "        if src_node_id2links.get(link_src) is None:\n",
    "            src_node_id2links[link_src] = []\n",
    "        src_node_id2links[link_src].append(link_dict)\n",
    "    \n",
    "    graph_s = \"\"\n",
    "    for n_id, node_dict in enumerate(nodes):\n",
    "        links = src_node_id2links.get(n_id, list())\n",
    "        start_label = node_dict[\"label\"]\n",
    "        if node_dict[\"type\"] == \"ANSWER_CANDIDATE_ENTITY\":\n",
    "            start_label = f\"{sep_token} {start_label} {sep_token}\"\n",
    "        for link_dict in links:\n",
    "            target_label = nodes[link_dict[\"target\"]][\"label\"]\n",
    "            if nodes[link_dict[\"target\"]][\"type\"] == \"ANSWER_CANDIDATE_ENTITY\":\n",
    "                target_label = f\"{sep_token} {target_label} {sep_token}\"\n",
    "            link_s = f\" {start_label}, {link_dict['label']}, {target_label} \"\n",
    "            graph_s += link_s\n",
    "    return graph_s\n",
    "\n",
    "class TextGraphDataset(Dataset):\n",
    "    def __init__(self,  tokenizer, max_length, split='train', include_graph=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "        self.include_graph = include_graph\n",
    "        if split in ['train', 'val', 'test']:\n",
    "            df = pd.read_csv(train_path, sep='\\t')\n",
    "            df[\"label\"] = df[\"correct\"].astype(np.float32)\n",
    "            self.df = self._split_train_dev_test(df, split)\n",
    "        elif split == 'full': # use this to use all data for training (before submit)\n",
    "            self.df = pd.read_csv(train_path, sep='\\t')\n",
    "            self.df[\"label\"] = self.df[\"correct\"].astype(np.float32)\n",
    "        elif split == 'eval': # this corresponds to submit\n",
    "            self.df = pd.read_csv(test_path, sep='\\t')\n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized split!\")\n",
    "        \n",
    "        self.questions = []\n",
    "        self.q_entities = []\n",
    "        self.a_entities = []\n",
    "        self.graphs = []\n",
    "        self.labels = []\n",
    "\n",
    "        self._get_data()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q_entities = self.q_entities[idx] + ':'\n",
    "        question = self.questions[idx]\n",
    "        a_entities = self.a_entities[idx]\n",
    "        \n",
    "        if self.include_graph:\n",
    "            raise NotImplementedError(\"Need to append graph in text form to answer\")\n",
    "        \n",
    "        try: \n",
    "            tokenizer_out = self.tokenizer.encode_plus(\n",
    "                text=q_entities + ' ' + question,\n",
    "                text_pair=a_entities,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=\"only_first\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        except Exception:\n",
    "            print(question, q_entities, a_entities)\n",
    "\n",
    "        res = {\n",
    "            \"input_ids\": tokenizer_out[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": tokenizer_out[\"attention_mask\"].flatten(),\n",
    "        }\n",
    "        \n",
    "        if self.split != \"eval\":\n",
    "            res[\"labels\"] = self.labels[idx]\n",
    "        \n",
    "        if \"token_type_ids\" in tokenizer_out:\n",
    "            res[\"token_type_ids\"] = tokenizer_out[\"token_type_ids\"].flatten()\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def _get_data(self):\n",
    "        for idx, data in self.df.iterrows():\n",
    "            self.questions.append(data[\"question\"])\n",
    "            self.q_entities.append(data[\"questionEntity\"])\n",
    "            self.a_entities.append(data[\"answerEntity\"])\n",
    "            if self.split != \"eval\":\n",
    "                self.labels.append(data[\"label\"])\n",
    "            if self.include_graph:\n",
    "                self.graphs.append(data[\"graph\"].apply(eval))\n",
    "\n",
    "    def _split_train_dev_test(self, df, split='train'):\n",
    "        all_questions = list(df[\"question\"].unique())\n",
    "        num_questions = len(all_questions)\n",
    "        random.shuffle(all_questions)\n",
    "\n",
    "        train_dev_ratio = 0.8\n",
    "        train_ratio = 0.9\n",
    "        num_train_dev_questions = int(num_questions * train_dev_ratio)\n",
    "        train_dev_questions = all_questions[:num_train_dev_questions]\n",
    "        test_questions = set(all_questions[num_train_dev_questions:])\n",
    "        num_train_questions = int(len(train_dev_questions) * train_ratio)\n",
    "        train_questions = set(train_dev_questions[:num_train_questions])\n",
    "        dev_questions = set(train_dev_questions[num_train_questions:])\n",
    "\n",
    "        train_df = df[df[\"question\"].isin(train_questions)]\n",
    "        dev_df = df[df[\"question\"].isin(dev_questions)]\n",
    "        test_df = df[df[\"question\"].isin(test_questions)]\n",
    "\n",
    "        if split == 'train':\n",
    "            return train_df\n",
    "        elif split =='val':\n",
    "            return dev_df\n",
    "        else:\n",
    "            return test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prep and finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "model_name = \"whaleloops/phrase-bert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pretrained_bert = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_bert):\n",
    "        super().__init__()\n",
    "        self.bert_backbone = pretrained_bert\n",
    "        self.hidden_size = pretrained_bert.config.hidden_size\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.hidden_size // 2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert_backbone(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        last_hidden_state = outputs.last_hidden_state  # Access the last hidden states\n",
    "        pooled_output = last_hidden_state[:, 0, :]  # Take the [CLS] token representation\n",
    "        logits = self.head(pooled_output)\n",
    "        return logits\n",
    "    \n",
    "model = QuestionClassifier(\n",
    "    pretrained_bert\n",
    ").to(DEVICE)\n",
    "\n",
    "for p in model.bert_backbone.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install peft -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, LoraModel\n",
    "\n",
    "LORA_RANK=16\n",
    "LORA_ALPHA=32.\n",
    "LORA_DROPOUT=1e-1\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    use_rslora=True,\n",
    ")\n",
    "\n",
    "lora_model = LoraModel(model, config, \"default\")\n",
    "\n",
    "for p in lora_model.head.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_trainable_params(model: nn.Module):\n",
    "    params = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            params.append(p)\n",
    "    return params\n",
    "\n",
    "trainable_params = get_trainable_params(lora_model)\n",
    "len(trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "\n",
    "    avg_loss = 0.\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for i, batch in enumerate(loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE).float()\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids).squeeze()\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        with torch.no_grad():\n",
    "            preds = F.sigmoid(logits).detach().cpu().numpy()\n",
    "            preds = (preds > 0.5) * 1\n",
    "            y_true = labels.detach().cpu().numpy()\n",
    "            \n",
    "            predictions += preds.tolist()\n",
    "            true_labels += y_true.tolist()\n",
    "    \n",
    "    avg_loss /= len(loader) + 1\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)    \n",
    "    \n",
    "    return avg_loss, f1, precision, recall\n",
    "\n",
    "\n",
    "@torch.no_grad\n",
    "def eval_epoch(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "\n",
    "    avg_loss = 0.\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    for i, batch in enumerate(loader):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE).float()\n",
    "        \n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids).squeeze()\n",
    "        loss = loss_fn(logits, labels)\n",
    "        \n",
    "        avg_loss += loss.item()\n",
    "        preds = F.sigmoid(logits).detach().cpu().numpy()\n",
    "        preds = (preds > 0.5) * 1\n",
    "        y_true = labels.detach().cpu().numpy()\n",
    "        predictions += preds.tolist()\n",
    "        true_labels += y_true.tolist()\n",
    "\n",
    "    avg_loss /= len(loader)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "\n",
    "    return avg_loss, f1, precision, recall\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, loss_fn, epochs=10):\n",
    "    for e in range(epochs):\n",
    "        loss, f1, prec, rec = train_epoch(model, train_loader, optimizer, loss_fn)\n",
    "        print(f\"Train epoch {e + 1} - loss: {loss:.3f}, f1: {f1:.3f}, precision: {prec:.3f}, recall: {rec:.3f}\")\n",
    "        \n",
    "        loss, f1, prec, rec = eval_epoch(model, val_loader, loss_fn)\n",
    "        print(f\"Eval epoch {e + 1} - loss: {loss:.3f}, f1: {f1:.3f}, precision: {prec:.3f}, recall: {rec:.3f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, evaluation and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "MAX_LENGTH=150\n",
    "EPOCHS=50\n",
    "\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "train_ds = TextGraphDataset(tokenizer, MAX_LENGTH, split='train')\n",
    "dev_ds = TextGraphDataset(tokenizer, MAX_LENGTH, split='val')\n",
    "test_ds = TextGraphDataset(tokenizer, MAX_LENGTH, split='test')\n",
    "\n",
    "weights = compute_sample_weight('balanced', train_ds.labels)\n",
    "sampler = WeightedRandomSampler(weights, len(weights)) # we will oversample correct answers :)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "#train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "dev_loader = DataLoader(dev_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(params=trainable_params, lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 1 - loss: 0.636, f1: 0.637, precision: 0.618, recall: 0.657\n",
      "Eval epoch 1 - loss: 0.565, f1: 0.286, precision: 0.180, recall: 0.696\n",
      "Train epoch 2 - loss: 0.550, f1: 0.732, precision: 0.687, recall: 0.784\n",
      "Eval epoch 2 - loss: 0.757, f1: 0.285, precision: 0.171, recall: 0.870\n",
      "Train epoch 3 - loss: 0.509, f1: 0.766, precision: 0.717, recall: 0.822\n",
      "Eval epoch 3 - loss: 0.533, f1: 0.340, precision: 0.215, recall: 0.813\n",
      "Train epoch 4 - loss: 0.473, f1: 0.791, precision: 0.738, recall: 0.853\n",
      "Eval epoch 4 - loss: 0.523, f1: 0.352, precision: 0.221, recall: 0.873\n",
      "Train epoch 5 - loss: 0.459, f1: 0.800, precision: 0.751, recall: 0.856\n",
      "Eval epoch 5 - loss: 0.438, f1: 0.404, precision: 0.269, recall: 0.809\n",
      "Train epoch 6 - loss: 0.416, f1: 0.820, precision: 0.778, recall: 0.867\n",
      "Eval epoch 6 - loss: 0.422, f1: 0.434, precision: 0.299, recall: 0.786\n",
      "Train epoch 7 - loss: 0.390, f1: 0.837, precision: 0.794, recall: 0.885\n",
      "Eval epoch 7 - loss: 0.415, f1: 0.432, precision: 0.296, recall: 0.803\n",
      "Train epoch 8 - loss: 0.373, f1: 0.850, precision: 0.809, recall: 0.896\n",
      "Eval epoch 8 - loss: 0.431, f1: 0.447, precision: 0.309, recall: 0.806\n",
      "Train epoch 9 - loss: 0.363, f1: 0.849, precision: 0.809, recall: 0.893\n",
      "Eval epoch 9 - loss: 0.357, f1: 0.480, precision: 0.339, recall: 0.823\n",
      "Train epoch 10 - loss: 0.345, f1: 0.862, precision: 0.824, recall: 0.903\n",
      "Eval epoch 10 - loss: 0.386, f1: 0.473, precision: 0.329, recall: 0.843\n",
      "Train epoch 11 - loss: 0.345, f1: 0.861, precision: 0.821, recall: 0.905\n",
      "Eval epoch 11 - loss: 0.357, f1: 0.484, precision: 0.348, recall: 0.796\n",
      "Train epoch 12 - loss: 0.331, f1: 0.868, precision: 0.830, recall: 0.909\n",
      "Eval epoch 12 - loss: 0.445, f1: 0.433, precision: 0.290, recall: 0.853\n",
      "Train epoch 13 - loss: 0.318, f1: 0.872, precision: 0.836, recall: 0.912\n",
      "Eval epoch 13 - loss: 0.374, f1: 0.494, precision: 0.351, recall: 0.829\n",
      "Train epoch 14 - loss: 0.304, f1: 0.879, precision: 0.843, recall: 0.918\n",
      "Eval epoch 14 - loss: 0.455, f1: 0.516, precision: 0.372, recall: 0.843\n",
      "Train epoch 15 - loss: 0.310, f1: 0.876, precision: 0.843, recall: 0.912\n",
      "Eval epoch 15 - loss: 0.305, f1: 0.535, precision: 0.397, recall: 0.816\n",
      "Train epoch 16 - loss: 0.299, f1: 0.882, precision: 0.850, recall: 0.916\n",
      "Eval epoch 16 - loss: 0.382, f1: 0.502, precision: 0.359, recall: 0.829\n",
      "Train epoch 17 - loss: 0.292, f1: 0.886, precision: 0.855, recall: 0.920\n",
      "Eval epoch 17 - loss: 0.399, f1: 0.489, precision: 0.343, recall: 0.856\n",
      "Train epoch 18 - loss: 0.296, f1: 0.882, precision: 0.850, recall: 0.916\n",
      "Eval epoch 18 - loss: 0.302, f1: 0.541, precision: 0.411, recall: 0.793\n",
      "Train epoch 19 - loss: 0.291, f1: 0.882, precision: 0.852, recall: 0.915\n",
      "Eval epoch 19 - loss: 0.383, f1: 0.510, precision: 0.371, recall: 0.816\n",
      "Train epoch 20 - loss: 0.297, f1: 0.885, precision: 0.854, recall: 0.918\n",
      "Eval epoch 20 - loss: 0.371, f1: 0.491, precision: 0.349, recall: 0.823\n",
      "Train epoch 21 - loss: 0.291, f1: 0.887, precision: 0.856, recall: 0.920\n",
      "Eval epoch 21 - loss: 0.449, f1: 0.449, precision: 0.305, recall: 0.853\n",
      "Train epoch 22 - loss: 0.289, f1: 0.887, precision: 0.857, recall: 0.920\n",
      "Eval epoch 22 - loss: 0.289, f1: 0.560, precision: 0.447, recall: 0.749\n",
      "Train epoch 23 - loss: 0.283, f1: 0.889, precision: 0.862, recall: 0.919\n",
      "Eval epoch 23 - loss: 0.328, f1: 0.532, precision: 0.399, recall: 0.796\n",
      "Train epoch 24 - loss: 0.281, f1: 0.891, precision: 0.863, recall: 0.920\n",
      "Eval epoch 24 - loss: 0.397, f1: 0.498, precision: 0.356, recall: 0.829\n",
      "Train epoch 25 - loss: 0.274, f1: 0.894, precision: 0.867, recall: 0.923\n",
      "Eval epoch 25 - loss: 0.347, f1: 0.554, precision: 0.423, recall: 0.803\n",
      "Train epoch 26 - loss: 0.282, f1: 0.890, precision: 0.863, recall: 0.918\n",
      "Eval epoch 26 - loss: 0.402, f1: 0.491, precision: 0.348, recall: 0.829\n",
      "Train epoch 27 - loss: 0.279, f1: 0.892, precision: 0.862, recall: 0.924\n",
      "Eval epoch 27 - loss: 0.356, f1: 0.505, precision: 0.361, recall: 0.839\n",
      "Train epoch 28 - loss: 0.279, f1: 0.891, precision: 0.863, recall: 0.921\n",
      "Eval epoch 28 - loss: 0.313, f1: 0.559, precision: 0.431, recall: 0.796\n",
      "Train epoch 29 - loss: 0.275, f1: 0.893, precision: 0.867, recall: 0.921\n",
      "Eval epoch 29 - loss: 0.368, f1: 0.535, precision: 0.402, recall: 0.799\n",
      "Train epoch 30 - loss: 0.257, f1: 0.900, precision: 0.876, recall: 0.925\n",
      "Eval epoch 30 - loss: 0.375, f1: 0.541, precision: 0.403, recall: 0.823\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    lora_model,\n",
    "    train_loader,\n",
    "    dev_loader,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on hold-out test - f1: 0.53, precision: 0.39, recall: 0.82\n"
     ]
    }
   ],
   "source": [
    "_, f1, prec, rec = eval_epoch(model, test_loader, loss_fn)\n",
    "print(f\"Performance on hold-out test - f1: {f1:.2f}, precision: {prec:.2f}, recall: {rec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"phrase_bert_lora_fixed_oversampling.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved model (maybe there is a better way with PEFT around, but...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ds = TextGraphDataset(tokenizer, max_length=MAX_LENGTH, split='eval')\n",
    "eval_df = eval_ds.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After publishing A Time to Kill, which book did its author begin working on immediately?\n",
      "After publishing A Time to Kill, which book did its author begin working on immediately?\n",
      "After publishing A Time to Kill, which book did its author begin working on immediately?\n",
      "After publishing A Time to Kill, which book did its author begin working on immediately?\n",
      "After publishing A Time to Kill, which book did its author begin working on immediately?\n",
      "After publishing A Time to Kill, which book did its author begin working on immediately?\n",
      "After publishing A Time to Kill, which book did its author begin working on immediately?\n",
      "After publishing A Time to Kill, which book did its author begin working on immediately?\n",
      "After publishing A Time to Kill, which book did its author begin working on immediately?\n",
      "After publishing A Time to Kill, which book did its author begin working on immediately?\n",
      "After publishing A Time to Kill, which book did its author begin working on immediately?\n"
     ]
    }
   ],
   "source": [
    "q = eval_df.loc[0, \"question\"]\n",
    "ids = eval_df.index[eval_df['question'] == q].tolist()\n",
    "for id in ids:\n",
    "    print(eval_ds.questions[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def make_submit_predictions(model, tokenizer, filename='test_result_1.tsv'):\n",
    "    model.eval()\n",
    "    eval_ds = TextGraphDataset(tokenizer, max_length=MAX_LENGTH, split='eval')\n",
    "    preds = []\n",
    "    for idx, data in enumerate(eval_ds):\n",
    "        input_ids = data[\"input_ids\"].to(DEVICE).unsqueeze(0)\n",
    "        attention_mask = data[\"attention_mask\"].to(DEVICE).unsqueeze(0)\n",
    "        token_type_ids = data[\"token_type_ids\"].to(DEVICE).unsqueeze(0)\n",
    "        \n",
    "        logit = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids).squeeze()\n",
    "        pred = (logit.detach().cpu().numpy() > 0) * 1\n",
    "        preds.append(pred)\n",
    "\n",
    "    df = eval_ds.df\n",
    "    df['prediction'] = preds\n",
    "    df['prediction'] = df['prediction'].astype(int)\n",
    "    df[[\"sample_id\", \"prediction\"]].to_csv(filename, sep='\\t', index=False)\n",
    "\n",
    "@torch.no_grad\n",
    "def make_submit_predictions_ranked(model, tokenizer, filename='test_result_2.tsv'):\n",
    "    \"\"\"based of Vika's idea - select all candidate answers for questions, select one with max prob\"\"\"\n",
    "    model.eval()\n",
    "    eval_ds = TextGraphDataset(tokenizer, max_length=MAX_LENGTH, split='eval')\n",
    "    eval_df = eval_ds.df\n",
    "    eval_df[\"correct\"] = False\n",
    "\n",
    "    for question in eval_df['question'].unique():\n",
    "        ids = eval_df.index[eval_df['question'] == question].tolist()\n",
    "        \n",
    "        logits = []\n",
    "        for idx in ids:\n",
    "            data = eval_ds[idx]\n",
    "            input_ids = data[\"input_ids\"].to(DEVICE).unsqueeze(0)\n",
    "            attention_mask = data[\"attention_mask\"].to(DEVICE).unsqueeze(0)\n",
    "            token_type_ids = data[\"token_type_ids\"].to(DEVICE).unsqueeze(0)\n",
    "            \n",
    "            logit = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids).squeeze()\n",
    "            logits.append(logit.detach().cpu().item())\n",
    "\n",
    "        right_ans_id = ids[np.argmax(logits)]\n",
    "        eval_df.loc[right_ans_id, 'correct'] = True\n",
    "\n",
    "    eval_df['prediction'] = eval_df['correct']\n",
    "    eval_df['prediction'] = eval_df['prediction'].astype(int)\n",
    "    eval_df[[\"sample_id\", \"prediction\"]].to_csv(filename, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submit_predictions(\n",
    "    model,\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submit_predictions_ranked(\n",
    "    model,\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete retrain on full data (for best result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "MAX_LENGTH=150\n",
    "EPOCHS=50\n",
    "\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "ds = TextGraphDataset(tokenizer, MAX_LENGTH, split='full')\n",
    "weights = compute_sample_weight('balanced', train_ds.labels)\n",
    "sampler = WeightedRandomSampler(weights, len(weights)) # we will oversample correct answers :)\n",
    "loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 14 - loss: 0.193, f1: 0.930, precision: 0.904, recall: 0.957\n",
      "Train epoch 15 - loss: 0.186, f1: 0.930, precision: 0.903, recall: 0.959\n",
      "Train epoch 16 - loss: 0.186, f1: 0.932, precision: 0.907, recall: 0.959\n",
      "Train epoch 17 - loss: 0.180, f1: 0.934, precision: 0.909, recall: 0.960\n",
      "Train epoch 18 - loss: 0.180, f1: 0.937, precision: 0.914, recall: 0.960\n",
      "Train epoch 19 - loss: 0.170, f1: 0.937, precision: 0.913, recall: 0.962\n",
      "Train epoch 20 - loss: 0.168, f1: 0.938, precision: 0.916, recall: 0.961\n",
      "Train epoch 21 - loss: 0.164, f1: 0.940, precision: 0.918, recall: 0.964\n",
      "Train epoch 22 - loss: 0.168, f1: 0.939, precision: 0.915, recall: 0.963\n",
      "Train epoch 23 - loss: 0.162, f1: 0.942, precision: 0.921, recall: 0.964\n",
      "Train epoch 24 - loss: 0.161, f1: 0.941, precision: 0.921, recall: 0.962\n",
      "Train epoch 25 - loss: 0.153, f1: 0.945, precision: 0.927, recall: 0.963\n",
      "Train epoch 26 - loss: 0.154, f1: 0.943, precision: 0.925, recall: 0.962\n",
      "Train epoch 27 - loss: 0.147, f1: 0.946, precision: 0.926, recall: 0.967\n",
      "Train epoch 28 - loss: 0.148, f1: 0.947, precision: 0.929, recall: 0.965\n",
      "Train epoch 29 - loss: 0.139, f1: 0.951, precision: 0.937, recall: 0.966\n",
      "Train epoch 30 - loss: 0.144, f1: 0.949, precision: 0.932, recall: 0.967\n",
      "Train epoch 31 - loss: 0.144, f1: 0.950, precision: 0.933, recall: 0.967\n",
      "Train epoch 32 - loss: 0.141, f1: 0.950, precision: 0.934, recall: 0.967\n",
      "Train epoch 33 - loss: 0.138, f1: 0.950, precision: 0.934, recall: 0.966\n",
      "Train epoch 34 - loss: 0.136, f1: 0.951, precision: 0.937, recall: 0.967\n",
      "Train epoch 35 - loss: 0.137, f1: 0.952, precision: 0.934, recall: 0.970\n",
      "Train epoch 36 - loss: 0.127, f1: 0.956, precision: 0.940, recall: 0.973\n",
      "Train epoch 37 - loss: 0.134, f1: 0.954, precision: 0.938, recall: 0.971\n",
      "Train epoch 38 - loss: 0.128, f1: 0.954, precision: 0.938, recall: 0.971\n",
      "Train epoch 39 - loss: 0.137, f1: 0.951, precision: 0.934, recall: 0.969\n",
      "Train epoch 40 - loss: 0.127, f1: 0.956, precision: 0.942, recall: 0.971\n",
      "Train epoch 41 - loss: 0.126, f1: 0.954, precision: 0.938, recall: 0.970\n",
      "Train epoch 42 - loss: 0.126, f1: 0.956, precision: 0.941, recall: 0.971\n",
      "Train epoch 43 - loss: 0.123, f1: 0.959, precision: 0.943, recall: 0.975\n",
      "Train epoch 44 - loss: 0.122, f1: 0.957, precision: 0.943, recall: 0.972\n",
      "Train epoch 45 - loss: 0.125, f1: 0.957, precision: 0.942, recall: 0.973\n",
      "Train epoch 46 - loss: 0.127, f1: 0.956, precision: 0.942, recall: 0.971\n",
      "Train epoch 47 - loss: 0.128, f1: 0.954, precision: 0.938, recall: 0.970\n",
      "Train epoch 48 - loss: 0.127, f1: 0.957, precision: 0.940, recall: 0.973\n",
      "Train epoch 49 - loss: 0.120, f1: 0.959, precision: 0.944, recall: 0.974\n",
      "Train epoch 50 - loss: 0.121, f1: 0.958, precision: 0.944, recall: 0.972\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPOCHS - 13):\n",
    "    loss, f1, prec, rec = train_epoch(model, loader, optimizer, loss_fn)\n",
    "    print(f\"Train epoch {e + 1 + 13} - loss: {loss:.3f}, f1: {f1:.3f}, precision: {prec:.3f}, recall: {rec:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"phrase_bert_lora_oversampling_full.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submit_predictions_ranked(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    \"test_res_overfit.csv\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
