{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beating Text-Graph-17 with only Text\n",
    "\n",
    "Current plan is following\n",
    "\n",
    "- preprocess text into **q-a connection prediction** (question + question entities [SEP] answer + answer entities (+ linear. graph))\n",
    "- finetune bert-like model (bigger=better) with some cool LoRA (this one needs to be tuned too)\n",
    "- abuse augmentations for upsampling minor \"correct\" label examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_789/4131136508.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.random.manual_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.cuda.random.manual_seed(SEED)\n",
    "torch.cuda.random.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/tsv/train.tsv'\n",
    "test_path = 'data/tsv/test.tsv'\n",
    "\n",
    "class TextGraphDataset(Dataset):\n",
    "    def __init__(self,  tokenizer, max_length, split='train',include_graph=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "        self.include_graph = include_graph\n",
    "        if split in ['train', 'val', 'test']:\n",
    "            df = pd.read_csv(train_path, sep='\\t')\n",
    "            df[\"label\"] = df[\"correct\"].astype(np.float32)\n",
    "            self.df = self._split_train_dev_test(df, split)\n",
    "        elif split == 'eval': # this corresponds to submit\n",
    "            self.df = pd.read_csv(test_path, sep='\\t')\n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized split!\")\n",
    "        \n",
    "        self.quetions = []\n",
    "        self.q_entities = []\n",
    "        self.a_entities = []\n",
    "        self.graphs = []\n",
    "        self.labels = []\n",
    "\n",
    "        self._get_data()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q_entities = self.q_entities[idx] + ':'\n",
    "        question = self.quetions[idx]\n",
    "        a_entities = self.a_entities[idx]\n",
    "        \n",
    "        if self.include_graph:\n",
    "            raise NotImplementedError(\"Need to append graph in text form to answer\")\n",
    "        \n",
    "        try: \n",
    "            tokenizer_out = self.tokenizer.encode_plus(\n",
    "                text=q_entities + ' ' + question,\n",
    "                text_pair=a_entities,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\",\n",
    "                truncation=\"only_first\",\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        except Exception:\n",
    "            print(question, q_entities, a_entities)\n",
    "\n",
    "        res = {\n",
    "            \"input_ids\": tokenizer_out[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": tokenizer_out[\"attention_mask\"].flatten(),\n",
    "        }\n",
    "        \n",
    "        if self.split != \"eval\":\n",
    "            res[\"labels\"] = self.labels[idx]\n",
    "        \n",
    "        if \"token_type_ids\" in tokenizer_out:\n",
    "            res[\"token_type_ids\"] = tokenizer_out[\"token_type_ids\"].flatten()\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def _get_data(self):\n",
    "        for idx, data in self.df.iterrows():\n",
    "            self.quetions.append(data[\"question\"])\n",
    "            self.q_entities.append(data[\"answerEntity\"])\n",
    "            self.a_entities.append(data[\"questionEntity\"])\n",
    "            if self.split != \"eval\":\n",
    "                self.labels.append(data[\"label\"])\n",
    "            if self.include_graph:\n",
    "                self.graphs.append(data[\"graph\"].apply(eval))\n",
    "\n",
    "    def _split_train_dev_test(self, df, split='train'):\n",
    "        all_questions = list(df[\"question\"].unique())\n",
    "        num_questions = len(all_questions)\n",
    "        random.shuffle(all_questions)\n",
    "\n",
    "        train_dev_ratio = 0.8\n",
    "        train_ratio = 0.9\n",
    "        num_train_dev_questions = int(num_questions * train_dev_ratio)\n",
    "        train_dev_questions = all_questions[:num_train_dev_questions]\n",
    "        test_questions = set(all_questions[num_train_dev_questions:])\n",
    "        num_train_questions = int(len(train_dev_questions) * train_ratio)\n",
    "        train_questions = set(train_dev_questions[:num_train_questions])\n",
    "        dev_questions = set(train_dev_questions[num_train_questions:])\n",
    "\n",
    "        train_df = df[df[\"question\"].isin(train_questions)]\n",
    "        dev_df = df[df[\"question\"].isin(dev_questions)]\n",
    "        test_df = df[df[\"question\"].isin(test_questions)]\n",
    "\n",
    "        if split == 'train':\n",
    "            return train_df\n",
    "        elif split =='dev_df':\n",
    "            return dev_df\n",
    "        else:\n",
    "            return test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prep and finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "model_name = \"whaleloops/phrase-bert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pretrained_bert = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_bert):\n",
    "        super().__init__()\n",
    "        self.bert_backbone = pretrained_bert\n",
    "        self.hidden_size = pretrained_bert.config.hidden_size\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.hidden_size // 2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert_backbone(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        last_hidden_state = outputs.last_hidden_state  # Access the last hidden states\n",
    "        pooled_output = last_hidden_state[:, 0, :]  # Take the [CLS] token representation\n",
    "        logits = self.head(pooled_output)\n",
    "        return logits\n",
    "    \n",
    "model = QuestionClassifier(\n",
    "    pretrained_bert\n",
    ").to(DEVICE)\n",
    "\n",
    "for p in model.bert_backbone.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install peft -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, LoraModel\n",
    "\n",
    "LORA_RANK=8\n",
    "LORA_ALPHA=10.\n",
    "LORA_DROPOUT=1e-2\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    ")\n",
    "\n",
    "lora_model = LoraModel(model, config, \"default\")\n",
    "\n",
    "for p in lora_model.head.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_trainable_params(model: nn.Module):\n",
    "    params = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            params.append(p)\n",
    "    return params\n",
    "\n",
    "trainable_params = get_trainable_params(lora_model)\n",
    "len(trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "\n",
    "    avg_loss = 0.\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for i, batch in enumerate(loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE).float()\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids).squeeze()\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        with torch.no_grad():\n",
    "            preds = F.sigmoid(logits).detach().cpu().numpy()\n",
    "            preds = (preds > 0.5) * 1\n",
    "            y_true = labels.detach().cpu().numpy()\n",
    "            \n",
    "            predictions += preds.tolist()\n",
    "            true_labels += y_true.tolist()\n",
    "    \n",
    "    avg_loss /= len(loader) + 1\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)    \n",
    "    \n",
    "    return avg_loss, f1, precision, recall\n",
    "\n",
    "\n",
    "@torch.no_grad\n",
    "def eval_epoch(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "\n",
    "    avg_loss = 0.\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    for i, batch in enumerate(loader):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE).float()\n",
    "        \n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids).squeeze()\n",
    "        loss = loss_fn(logits, labels)\n",
    "        \n",
    "        avg_loss += loss.item()\n",
    "        preds = F.sigmoid(logits).detach().cpu().numpy()\n",
    "        preds = (preds > 0.5) * 1\n",
    "        y_true = labels.detach().cpu().numpy()\n",
    "        predictions += preds.tolist()\n",
    "        true_labels += y_true.tolist()\n",
    "\n",
    "    avg_loss /= len(loader)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "\n",
    "    return avg_loss, f1, precision, recall\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, loss_fn, epochs=10):\n",
    "    for e in range(epochs):\n",
    "        loss, f1, prec, rec = train_epoch(model, train_loader, optimizer, loss_fn)\n",
    "        print(f\"Train epoch {e + 1} - loss: {loss:.3f}, f1: {f1:.3f}, precision: {prec:.3f}, recall: {rec:.3f}\")\n",
    "        \n",
    "        loss, f1, prec, rec = eval_epoch(model, val_loader, loss_fn)\n",
    "        print(f\"Eval epoch {e + 1} - loss: {loss:.3f}, f1: {f1:.3f}, precision: {prec:.3f}, recall: {rec:.3f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, evaluation and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "MAX_LENGTH=128\n",
    "EPOCHS=30\n",
    "\n",
    "train_ds = TextGraphDataset(tokenizer, MAX_LENGTH, split='train')\n",
    "dev_ds = TextGraphDataset(tokenizer, MAX_LENGTH, split='val')\n",
    "test_ds = TextGraphDataset(tokenizer, MAX_LENGTH, split='test')\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "dev_loader = DataLoader(dev_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(params=trainable_params, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8934"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 1 - loss: 0.322, f1: 0.001, precision: 0.080, recall: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval epoch 1 - loss: 0.305, f1: 0.000, precision: 0.000, recall: 0.000\n",
      "Train epoch 2 - loss: 0.308, f1: 0.002, precision: 0.500, recall: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval epoch 2 - loss: 0.297, f1: 0.000, precision: 0.000, recall: 0.000\n",
      "Train epoch 3 - loss: 0.300, f1: 0.011, precision: 0.600, recall: 0.006\n",
      "Eval epoch 3 - loss: 0.287, f1: 0.003, precision: 1.000, recall: 0.001\n",
      "Train epoch 4 - loss: 0.290, f1: 0.018, precision: 0.490, recall: 0.009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval epoch 4 - loss: 0.284, f1: 0.000, precision: 0.000, recall: 0.000\n",
      "Train epoch 5 - loss: 0.281, f1: 0.037, precision: 0.598, recall: 0.019\n",
      "Eval epoch 5 - loss: 0.269, f1: 0.013, precision: 1.000, recall: 0.007\n",
      "Train epoch 6 - loss: 0.273, f1: 0.054, precision: 0.567, recall: 0.028\n",
      "Eval epoch 6 - loss: 0.276, f1: 0.019, precision: 0.875, recall: 0.009\n",
      "Train epoch 7 - loss: 0.266, f1: 0.070, precision: 0.592, recall: 0.037\n",
      "Eval epoch 7 - loss: 0.258, f1: 0.045, precision: 0.773, recall: 0.023\n",
      "Train epoch 8 - loss: 0.258, f1: 0.099, precision: 0.607, recall: 0.054\n",
      "Eval epoch 8 - loss: 0.251, f1: 0.162, precision: 0.667, recall: 0.092\n",
      "Train epoch 9 - loss: 0.254, f1: 0.135, precision: 0.655, recall: 0.075\n",
      "Eval epoch 9 - loss: 0.247, f1: 0.070, precision: 0.771, recall: 0.037\n",
      "Train epoch 10 - loss: 0.247, f1: 0.152, precision: 0.602, recall: 0.087\n",
      "Eval epoch 10 - loss: 0.247, f1: 0.019, precision: 1.000, recall: 0.009\n",
      "Train epoch 11 - loss: 0.241, f1: 0.184, precision: 0.642, recall: 0.107\n",
      "Eval epoch 11 - loss: 0.244, f1: 0.114, precision: 0.849, recall: 0.061\n",
      "Train epoch 12 - loss: 0.236, f1: 0.200, precision: 0.597, recall: 0.120\n",
      "Eval epoch 12 - loss: 0.256, f1: 0.137, precision: 0.846, recall: 0.075\n",
      "Train epoch 13 - loss: 0.231, f1: 0.229, precision: 0.598, recall: 0.141\n",
      "Eval epoch 13 - loss: 0.236, f1: 0.100, precision: 0.886, recall: 0.053\n",
      "Train epoch 14 - loss: 0.224, f1: 0.251, precision: 0.638, recall: 0.156\n",
      "Eval epoch 14 - loss: 0.236, f1: 0.379, precision: 0.660, recall: 0.266\n",
      "Train epoch 15 - loss: 0.222, f1: 0.272, precision: 0.637, recall: 0.173\n",
      "Eval epoch 15 - loss: 0.233, f1: 0.393, precision: 0.610, recall: 0.290\n",
      "Train epoch 16 - loss: 0.216, f1: 0.326, precision: 0.664, recall: 0.216\n",
      "Eval epoch 16 - loss: 0.230, f1: 0.440, precision: 0.656, recall: 0.331\n",
      "Train epoch 17 - loss: 0.212, f1: 0.348, precision: 0.637, recall: 0.240\n",
      "Eval epoch 17 - loss: 0.219, f1: 0.448, precision: 0.683, recall: 0.334\n",
      "Train epoch 18 - loss: 0.206, f1: 0.383, precision: 0.649, recall: 0.271\n",
      "Eval epoch 18 - loss: 0.233, f1: 0.419, precision: 0.723, recall: 0.294\n",
      "Train epoch 19 - loss: 0.205, f1: 0.391, precision: 0.643, recall: 0.281\n",
      "Eval epoch 19 - loss: 0.223, f1: 0.261, precision: 0.918, recall: 0.152\n",
      "Train epoch 20 - loss: 0.199, f1: 0.417, precision: 0.660, recall: 0.305\n",
      "Eval epoch 20 - loss: 0.222, f1: 0.490, precision: 0.681, recall: 0.383\n",
      "Train epoch 21 - loss: 0.199, f1: 0.430, precision: 0.676, recall: 0.316\n",
      "Eval epoch 21 - loss: 0.216, f1: 0.422, precision: 0.791, recall: 0.288\n",
      "Train epoch 22 - loss: 0.193, f1: 0.443, precision: 0.672, recall: 0.331\n",
      "Eval epoch 22 - loss: 0.225, f1: 0.531, precision: 0.673, recall: 0.438\n",
      "Train epoch 23 - loss: 0.193, f1: 0.461, precision: 0.676, recall: 0.349\n",
      "Eval epoch 23 - loss: 0.229, f1: 0.587, precision: 0.544, recall: 0.636\n",
      "Train epoch 24 - loss: 0.186, f1: 0.476, precision: 0.683, recall: 0.365\n",
      "Eval epoch 24 - loss: 0.207, f1: 0.550, precision: 0.703, recall: 0.452\n",
      "Train epoch 25 - loss: 0.183, f1: 0.502, precision: 0.680, recall: 0.398\n",
      "Eval epoch 25 - loss: 0.217, f1: 0.423, precision: 0.866, recall: 0.280\n",
      "Train epoch 26 - loss: 0.181, f1: 0.504, precision: 0.694, recall: 0.395\n",
      "Eval epoch 26 - loss: 0.202, f1: 0.486, precision: 0.808, recall: 0.347\n",
      "Train epoch 27 - loss: 0.183, f1: 0.506, precision: 0.699, recall: 0.396\n",
      "Eval epoch 27 - loss: 0.217, f1: 0.499, precision: 0.743, recall: 0.376\n",
      "Train epoch 28 - loss: 0.179, f1: 0.521, precision: 0.696, recall: 0.416\n",
      "Eval epoch 28 - loss: 0.204, f1: 0.546, precision: 0.717, recall: 0.441\n",
      "Train epoch 29 - loss: 0.175, f1: 0.537, precision: 0.724, recall: 0.427\n",
      "Eval epoch 29 - loss: 0.213, f1: 0.592, precision: 0.689, recall: 0.520\n",
      "Train epoch 30 - loss: 0.176, f1: 0.538, precision: 0.704, recall: 0.435\n",
      "Eval epoch 30 - loss: 0.215, f1: 0.417, precision: 0.830, recall: 0.278\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    lora_model,\n",
    "    train_loader,\n",
    "    dev_loader,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on hold-out test - f1 - 0.44, precision: 0.82, recall: 0.30\n"
     ]
    }
   ],
   "source": [
    "_, f1, prec, rec = eval_epoch(model, test_loader, loss_fn)\n",
    "print(f\"Performance on hold-out test - f1: {f1:.2f}, precision: {prec:.2f}, recall: {rec:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"phrase_bert_lora_tuned.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def make_submit_predictions(model, tokenizer, filename='test_result_1.tsv'):\n",
    "    model.eval()\n",
    "    eval_ds = TextGraphDataset(tokenizer, max_length=MAX_LENGTH, split='eval')\n",
    "    preds = []\n",
    "    for idx, data in enumerate(eval_ds):\n",
    "        input_ids = data[\"input_ids\"].to(DEVICE).unsqueeze(0)\n",
    "        attention_mask = data[\"attention_mask\"].to(DEVICE).unsqueeze(0)\n",
    "        token_type_ids = data[\"token_type_ids\"].to(DEVICE).unsqueeze(0)\n",
    "        \n",
    "        logit = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids).squeeze()\n",
    "        pred = (logit.detach().cpu().numpy() > 0) * 1\n",
    "        preds.append(pred)\n",
    "\n",
    "    df = eval_ds.df\n",
    "    df['prediction'] = preds\n",
    "    df['prediction'] = df['prediction'].astype(int)\n",
    "    df[[\"sample_id\", \"prediction\"]].to_csv(filename, sep='\\t', index=False)\n",
    "\n",
    "@torch.no_grad\n",
    "def make_submit_predictions_ranked(model, tokenizer, filename='test_result_2.tsv'):\n",
    "    \"\"\"based of Vika's idea - select all candidate answers for questions, select one with max prob\"\"\"\n",
    "    model.eval()\n",
    "    eval_ds = TextGraphDataset(tokenizer, max_length=MAX_LENGTH, split='eval')\n",
    "    eval_df = eval_ds.df\n",
    "    eval_df[\"correct\"] = False\n",
    "\n",
    "    for question in eval_df['question'].unique():\n",
    "        ids = eval_df.index[eval_df['question'] == question].tolist()\n",
    "        \n",
    "        logits = []\n",
    "        for idx in ids:\n",
    "            data = eval_ds[idx]\n",
    "            input_ids = data[\"input_ids\"].to(DEVICE).unsqueeze(0)\n",
    "            attention_mask = data[\"attention_mask\"].to(DEVICE).unsqueeze(0)\n",
    "            token_type_ids = data[\"token_type_ids\"].to(DEVICE).unsqueeze(0)\n",
    "            \n",
    "            logit = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids).squeeze()\n",
    "            logits.append(logit.detach().cpu().item())\n",
    "\n",
    "        right_ans_id = ids[np.argmax(logits)]\n",
    "        eval_df.loc[right_ans_id, 'correct'] = True\n",
    "\n",
    "    eval_df['prediction'] = eval_df['correct']\n",
    "    eval_df['prediction'] = eval_df['prediction'].astype(int)\n",
    "    eval_df[[\"sample_id\", \"prediction\"]].to_csv(filename, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submit_predictions(\n",
    "    model,\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submit_predictions_ranked(\n",
    "    model,\n",
    "    tokenizer\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
