{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beating Text-Graph-17 with only Text\n",
    "\n",
    "Current plan is following\n",
    "\n",
    "- preprocess text into **q-a connection prediction** (question + question entities [SEP] answer + answer entities (+ linear. graph))\n",
    "- finetune bert-like model (bigger=better) with some cool LoRA (this one needs to be tuned too)\n",
    "- abuse augmentations for upsampling minor \"correct\" label examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.random.manual_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.cuda.random.manual_seed(SEED)\n",
    "torch.cuda.random.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/tsv/train.tsv'\n",
    "test_path = 'data/tsv/train.csv'\n",
    "\n",
    "class TextGraphDataset(Dataset):\n",
    "    def __init__(self,  tokenizer, max_length, split='train',include_graph=False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "        self.include_graph = include_graph\n",
    "        if split in ['train', 'val', 'test']:\n",
    "            df = pd.read_csv(train_path, sep='\\t')\n",
    "            df[\"label\"] = df[\"correct\"].astype(np.float32)\n",
    "            self.df = self._split_train_dev_test(df, split)\n",
    "        elif split == 'eval': # this corresponds to submit\n",
    "            self.df = pd.read_csv(test_path, sep='\\t')\n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized split!\")\n",
    "        \n",
    "        self.quetions = []\n",
    "        self.q_entities = []\n",
    "        self.a_entities = []\n",
    "        self.graphs = []\n",
    "        self.labels = []\n",
    "\n",
    "        self._get_data()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        q_entities = ', '.join(self.q_entities[idx]) + ':'\n",
    "        question = self.quetions[idx]\n",
    "        a_entities = ', '.join(self.a_entities[idx])\n",
    "        \n",
    "        if self.include_graph:\n",
    "            raise NotImplementedError(\"Need to append graph in text form to answer\")\n",
    "\n",
    "        tokenizer_out = self.tokenizer.encode_plus(\n",
    "            text=q_entities + ' ' + question,\n",
    "            text_pair=a_entities,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=\"only_first\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        res = {\n",
    "            \"input_ids\": tokenizer_out[\"input_ids\"][0],\n",
    "            \"attention_mask\": tokenizer_out[\"attention_mask\"][0],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "        if \"token_type_ids\" in tokenizer_out:\n",
    "            res[\"token_type_ids\"] = tokenizer_out[\"token_type_ids\"][0]\n",
    "        return res\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def _get_data(self):\n",
    "        for idx, data in self.df.iterrows():\n",
    "            self.quetions.append(data[\"question\"])\n",
    "            self.q_entities.append(data[\"answerEntity\"])\n",
    "            self.a_entities.append(data[\"questionEntity\"])\n",
    "            self.labels.append(data[\"label\"])\n",
    "            if self.include_graph:\n",
    "                self.graphs.append(data[\"graph\"].apply(eval))\n",
    "\n",
    "    def _split_train_dev_test(self, df, split='train'):\n",
    "        all_questions = list(df[\"question\"].unique())\n",
    "        num_questions = len(all_questions)\n",
    "        random.shuffle(all_questions)\n",
    "\n",
    "        train_dev_ratio = 0.8\n",
    "        train_ratio = 0.9\n",
    "        num_train_dev_questions = int(num_questions * train_dev_ratio)\n",
    "        train_dev_questions = all_questions[:num_train_dev_questions]\n",
    "        test_questions = set(all_questions[num_train_dev_questions:])\n",
    "        num_train_questions = int(len(train_dev_questions) * train_ratio)\n",
    "        train_questions = set(train_dev_questions[:num_train_questions])\n",
    "        dev_questions = set(train_dev_questions[num_train_questions:])\n",
    "\n",
    "        train_df = df[df[\"question\"].isin(train_questions)]\n",
    "        dev_df = df[df[\"question\"].isin(dev_questions)]\n",
    "        test_df = df[df[\"question\"].isin(test_questions)]\n",
    "\n",
    "        if split == 'train':\n",
    "            return train_df\n",
    "        elif split =='dev_df':\n",
    "            return dev_df\n",
    "        else:\n",
    "            return test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prep and finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "model_name = \"whaleloops/phrase-bert\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pretrained_bert = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_bert):\n",
    "        super().__init__()\n",
    "        self.bert_backbone = pretrained_bert\n",
    "        self.hidden_size = pretrained_bert.config.hidden_size\n",
    "        self.head = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert_backbone(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        last_hidden_state = outputs.last_hidden_state  # Access the last hidden states\n",
    "        pooled_output = last_hidden_state[:, 0, :]  # Take the [CLS] token representation\n",
    "        logits = self.head(pooled_output)\n",
    "        return logits\n",
    "    \n",
    "model = QuestionClassifier(\n",
    "    pretrained_bert\n",
    ").to(DEVICE)\n",
    "\n",
    "# freeze backbone\n",
    "for param in model.bert_backbone.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, LoraModel\n",
    "\n",
    "LORA_RANK=4\n",
    "LORA_ALPHA=100.\n",
    "LORA_DROPOUT=1e-2\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    r=LORA_RANK,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=LORA_DROPOUT\n",
    ")\n",
    "\n",
    "lora_model = LoraModel(model, config, \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_trainable_lora_params(model: nn.Module):\n",
    "    params = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            params.append(p)\n",
    "    return params\n",
    "\n",
    "lora_parameters = get_trainable_lora_params(model)\n",
    "len(lora_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "\n",
    "    avg_loss = 0.\n",
    "    avg_f1 = 0.\n",
    "    avg_precision = 0.\n",
    "    avg_recall = 0.\n",
    "    \n",
    "    for i, batch in enumerate(loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids).squeeze(1)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        with torch.no_grad():\n",
    "            preds = F.sigmoid(logits).detach().cpu().numpy()\n",
    "            preds = (preds > 0.5) * 1\n",
    "            y_true = labels.detach().cpu().numpy()\n",
    "            avg_f1 += f1_score(y_true, preds)\n",
    "            avg_precision += precision_score(y_true, preds)\n",
    "            avg_recall += recall_score(y_true, preds)\n",
    "\n",
    "        print(y_true)\n",
    "        print(preds)\n",
    "        break\n",
    "\n",
    "    avg_loss /= len(loader) + 1\n",
    "    avg_f1 /= len(loader) + 1\n",
    "    avg_precision /= len(loader) + 1\n",
    "    avg_recall /= len(loader) + 1\n",
    "    \n",
    "    return avg_loss, avg_f1, avg_precision, avg_recall\n",
    "\n",
    "\n",
    "@torch.no_grad\n",
    "def eval_epoch(model, loader, loss_fn):\n",
    "    model.eval()\n",
    "\n",
    "    avg_loss = 0.\n",
    "    avg_f1 = 0.\n",
    "    avg_precision = 0.\n",
    "    avg_recall = 0.\n",
    "\n",
    "    for i, batch in enumerate(loader):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        \n",
    "        logits = model(inputs=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids).squeeze(1)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        preds = F.sigmoid(logits).detach().cpu().numpy()\n",
    "        preds = (preds > 0.5) * 1\n",
    "        y_true = labels.detach().cpu().numpy()\n",
    "        avg_f1 += f1_score(y_true, preds)\n",
    "        avg_precision += precision_score(y_true, preds)\n",
    "        avg_recall += recall_score(y_true, preds)\n",
    "\n",
    "        avg_loss /= len(loader) + 1\n",
    "        avg_f1 /= len(loader) + 1\n",
    "        avg_precision /= len(loader) + 1\n",
    "        avg_recall /= len(loader) + 1\n",
    "\n",
    "    return avg_loss, avg_f1, avg_precision, avg_recall\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, loss_fn, epochs=10):\n",
    "    for e in range(epochs):\n",
    "        loss, f1, prec, recall = train_epoch(model, train_loader, optimizer, loss_fn)\n",
    "        print(f\"Train epoch {e + 1} - loss: {loss:.3f}, f1: {f1:.2f}, precision: {prec:.2f}, recall: {recall:.2f}\")\n",
    "\n",
    "        loss, f1, prec, recall = eval_epoch(model, val_loader, loss_fn)\n",
    "        print(f\"Eval epoch {e + 1} - loss: {loss:.3f}, f1: {f1:.2f}, precision: {prec:.2f}, recall: {recall:.2f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, evaluation and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "MAX_LENGTH=128\n",
    "EPOCHS=50\n",
    "\n",
    "train_ds = TextGraphDataset(tokenizer, MAX_LENGTH, split='train')\n",
    "dev_ds = TextGraphDataset(tokenizer, MAX_LENGTH, split='dev')\n",
    "test_ds = TextGraphDataset(tokenizer, MAX_LENGTH, split='test')\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "dev_loader = DataLoader(dev_ds,batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.functional.binary_cross_entropy_with_logits\n",
    "optimizer = optim.Adam(params=list(model.head.parameters())+lora_parameters, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    dev_loader,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    epochs=EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, f1, prec, recall = eval_epoch(model, test_loader, loss_fn)\n",
    "print(\"Performance on hold-out test - f1 - {f1:.2f}, precision - {prec:.2f}, recall - {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad\n",
    "def make_submit_predictions(model, tokenizer, filename='test_result.tsv'):\n",
    "    model.eval()\n",
    "    eval_ds = TextGraphDataset(tokenizer, max_length=MAX_LENGTH, split='eval')\n",
    "    preds = []\n",
    "    for idx, data in enumerate(eval_ds):\n",
    "        input_ids = data[\"input_ids\"].to(DEVICE).unsqueeze(0)\n",
    "        attention_mask = data[\"attention_mask\"].to(DEVICE).unsqueeze(0)\n",
    "        token_type_ids = data[\"token_type_ids\"].to(DEVICE).unsqueeze(0)\n",
    "        \n",
    "        logit = model(inputs=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids).squeeze(1)\n",
    "        pred = (logit.detach().cpu().numpy() > 0) * 1\n",
    "        preds.append(pred)\n",
    "\n",
    "    df = eval_ds.df\n",
    "    df['prediction'] = preds\n",
    "    df['prediction'] = df['prediction'].astype(int)\n",
    "    df[[\"sample_id\", \"prediction\"]].to_csv(filename, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submit_predictions(\n",
    "    model,\n",
    "    tokenizer\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-std",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
